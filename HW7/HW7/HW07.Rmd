---
title: "HW 07"
author: "Jiawei Hao, hjiawei"
date: "Due 2021-11-04 at midnight"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(boot)
set.seed(303203)
```


## Question 1 (3 pts)

Set your working directory using Session -> Set Working Directory -> To Source File.

Consider sampling $n$ pairs $(Y_i, X_i)$ from a very large population of size $N$. We will assume that the population is so large that we can treat $n/N \approx 0$, so that all pairs in our sample are effectively independent.

```{r}
xy <- read.csv("xy.csv")
ggplot(xy, aes(x = x, y = y)) + geom_point()
```

For the population, you want to relate $Y$ and $X$ as a linear function:
$$Y_i = \beta_0 + \beta_1 X_i + R_i$$
where 
\[
\begin{aligned}
\beta_1 &= \frac{\text{Cov}(X,Y)}{\text{Var}(X)} \\
\beta_0 &= E(Y) - \beta_1 E(X) \\
R_i &= Y_i - \beta_0 - \beta_1 X_i
\end{aligned}
\]

The the line described by $\beta_0$ and $\beta_1$ is the "population regression line". We don't get to observe $R_i$ for our sample, but we can estimate $\beta_0$ and $\beta_1$ to get estimates of $R_i$.



### Part (a) (1 pt)

The `lm` function in R can estimate $\beta_0$ and $\beta_1$ using sample means and variances. Since these estimators are based on sample means, we can use the **central limit theorem** to justify confidence intervals for $\beta_0$ and $\beta_1$ (we won't do so rigorously in this setting).

Use the `lm` function to estimate $\beta_0$ and $\beta_1$. Apply the `confint` function to the results to get 95% confidence intervals for the $\beta_1$ parameter.

The estimated residuals ($\hat R_i$) can be found by applying the `resid` function to the result of `lm`. Provide a density plot of these values (see `geom_density`). Do they give you any reason to be concerned about the validity of the Central Limit Theorem approximation?

```{r}
(linear_model <- lm(y~x, data = xy))
conf <- confint(linear_model, level = 0.95)[1:2]
residuals <- resid(linear_model)
z = data.frame(residuals)

ggplot(z, aes(x = residuals)) + geom_density()
```  
  
**Answer**: The graph is right skewed. These data seem to comes from a distribution that is neither bell shaped nor normally distributed. We may not have big enough sample size to ensure the independence in our data. Thus the central limit theorem may not hold for our case. 

### Part (b) (2 pts)

You can use the `coef` function to get just the estimators $\hat \beta_0$ and $\hat \beta_1$. Use the `boot` package to get basic and percentile confidence intervals for just $\beta_1$. You will need to write a custom function to give as the `statistic` argument to `boot`. Use at least 1000 bootstrap samples. You can use `boot.ci` for the confidence intervals.

Comment on the assumptions required for the bootstrap intervals.

```{r}
coef(linear_model)  

coef_x_stat <- function(x, index) {
  df <- x[index,]
  coef(lm(y~x, data = df))["x"]
}

coef_boot <- boot(xy, statistic = coef_x_stat, R = 1000)
boot.ci(boot.out = coef_boot, type = "basic")  
boot.ci(boot.out = coef_boot, type = "perc")
```   
  
**Answer**: Our assumptions are:   
1, T ≈ θ (n is large)  
2, F ≈ F (n is large)  
3, $F^*$≈ F (B is large)  

## Question 2 (6 pts)

Suppose that instead of sampling pairs, we first identified some important values of $x$ that we wanted to investigate. Treating these values as fixed, we sampled a varying number of $Y_i$ for each $x$ value. For these data, we'll attempt to model the conditional distribution of $Y \, | \, x$ as:
$$Y \, | \, x = \beta_0 + \beta_1 x + \epsilon$$
where $\epsilon$ epsilon is assumed to be symmetric about zero (therefore, $E(\epsilon) = 0$) and the variance of $\epsilon$ does not depend on $x$ (a property called "homoskedasticity"). These assumptions are very similar to the population regression line model (as $E(R_i) = 0$ by construction), but cover the case where we want to design the study on particular values (a common case is a randomized trial where $x$ values are assigned from a known procedure and $Y$ is measured after).

### Part (a) (3 pts)

Let's start with some stronger assumptions and then relax them in the subsequent parts of the question.

The assumptions that support the Central Limit Theorem in Question 1 can also be used to assume that $\epsilon \sim N(0, \sigma^2)$ so that:

$$Y \mid x \sim N(\beta_0 + \beta_1 x, \sigma^2)$$

We've noticed that the Normal distribution has "light tails" and assumptions based on Normality can be sensitive to outliers.

Instead, suppose we we model $\epsilon$ with a scaled $t$-distribution with 4 degrees of freedom (i.e., has fatter tails than the Normal distribution): 
$$\epsilon \sim \frac{\sigma}{\sqrt{2}} t(4) \Rightarrow \text{Var}(\epsilon) = \sigma^2$$
(The $\sqrt{2}$ is there just to scale the $t$-distribution to have a variance of 1. More generally, if we picked a differed degrees of freedom parameter $v$, this would be replaced with $\sqrt{v/(v-2)}$.)


One way to get an estimate of the distribution of $\hat \beta_1$ is the following algorithm:


1. Estimate $\beta_0$, $\beta_1$, and $\sigma$ using linear regression (you can get the $\hat \sigma$ using `summary(model)$sigma`),
2. For all the $x_i$ in the sample, generate $\hat y_i = \hat \beta_0 + \hat \beta_1 x_i$ (you can use `predict(model)` to get $\hat y$)
3. For $B$ replications, generate $Y_i^* = \hat y_i + \epsilon_i*$, where 
$$\epsilon^* \sim \frac{\hat \sigma}{\sqrt{2}} t(4)$$
4.  For each replication, use linear regression to estimate $\hat \beta_1^*$. 
5.  Use the $\alpha/2$ and $1 - \alpha/2$ quantiles of the bootstrap distribution to get the confidence intervals:
$$[2 \hat \beta_1 - \hat \beta_1^*(1 - \alpha/2), 2 \hat \beta_1 - \hat \beta_1^*(\alpha/2)]$$
To avoid double subscripts I've written $\hat \beta^*_1(1 - \alpha/2)$ as the upper $1 - \alpha/2$ quantile of the bootstrap (and likewise for the lower $\alpha/2$ quantile).

You may note that this is a "basic" basic bootstrap interval. In fact, this procedure (fitting parameters, then simulating from a model) is known as a **parametric bootstrap**.

Use the algorithm above to generate a confidence interval for $\beta_1$. Compare it to the fully parametric interval produced in Question 1(a). Which is larger or smaller?

Note: The `boot` function does have the option of performing a parametric bootstrap using a user supplied `rand.gen` function. Feel free to use this functionality, but you may find it easier to implement the algorithm directly.

```{r}
Xs <- xy["x"]
#1
coef(linear_model)
sigma <- summary(linear_model)$sigma
#2
y_hats <- predict(linear_model)

#3
Capital_Y <- function(x){
  x = x + sigma * rt(1,4) / sqrt(2)
}
boot_samples <- rerun(1000, map_dbl(y_hats, ~ Capital_Y(.x)))

#4
betas <- numeric(1000)
for (i in 1:1000){
  Ys <- boot_samples[i]
  temp_frame <- data.frame(Xs, Ys)
  colnames(temp_frame)[2] <- ("y")
  betas[i] <- coef(lm(y~x, data = temp_frame))["x"]
}

#5
q <- quantile(betas, c(0.025, 0.975))
(intervals <- c(2*coef(linear_model)["x"] - q[2], 2*coef(linear_model)["x"] - q[1]))


```

### Part (b) (2 pts)

As an alternative to sampling from an assumed distribution for $\epsilon$, we can replace step (3) in the previous algorithm with 

3. Draw a sample (with replacement) from $\hat \epsilon_i$ and make $Y_i^* = \hat y_i + \epsilon_i^*$

Implement this version of a parametric bootstrap. Feel free to use the `boot` package. 

```{r}
ehats <- xy$y - y_hats

e_star <- replicate(1000, sample(ehats, size = nrow(xy), replace = TRUE))

betas_partb <- numeric(1000)

for (i in 1:1000){
  Y_star <- y_hats+e_star[,i]
  betas_partb[i] <- coef(lm(Y_star~xy$x))[2]
}

q1 <- quantile(betas_partb, c(0.025, 0.975))
(intervals1 <- c(2*coef(linear_model)["x"] - q1[2], 2*coef(linear_model)["x"] - q1[1]))
```

### Part (c) (1 pt)

Discuss the differences in the four types of intervals we created (fully parametric in 1(a), non-parametric bootstrap in 1(b), two variations of parametric bootstrap in 2(a) and 2(b)). When analyzing a particular data set, when would you pick one method over the another methods?

**Answer:** The intervals I found in 1(b), 2(a), and 2(b) are relatively similar range while the interval for 1(a)'s interval seems off significantly, which implies fully parametric is not ideal for our sample in this situation. Intervals in 1(b), 2(a), and 2(b) are also comparable since some has a larger range and some has smaller range. We centainly prefer those with smaller interval range since it can give better estimation on beta. In my case, the non-parametric bootstrap in 1(b) seems to have the smallest confidence interval compare to 2(a) and 2(b).

## Question 3 (1 pts)

Read the paper "THE RISK OF CANCER ASSOCIATED WITH SPECIFIC MUTATIONS OF BRCA1 AND BRCA2 AMONG ASHKENAZI JEWS." Briefly summarize the paper. Make sure to discuss the research question, data source, methods, and results. How did the authors use the bootstrap procedure in this paper?

**Answer**: Research conducted at Washington DC Jewish community too examine the factors influencing the risks of having breast cancer. The research focus on whether the Jewish carriers of germ-line mutations in BRCA1 and BRCA2 faces higher risks of having breast cancer. Data source are from 5331 persons who completed the questionnaire and provided a blood sample. Many statistical methods were used in this research. Including confidence intervals, finger-stick procedures, allele-specific oligonucleotide assays for detection. The results compare risks of having breast cancer between mutations carriers and noncarriers and found that mutation carriers have higher chances of having breast cancer, especially at higher ages. Bootstrap procedures are used as author constructed 95% confidence interval using 1000 random samplings of the data with  replacement.

