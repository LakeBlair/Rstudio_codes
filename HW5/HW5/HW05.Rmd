---
title: "STATS 406, Homework 5"
author: "Jiawei Hao, hjiawei"
date: "Due October 5 at midnight"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
set.seed(393993931)
```



## Problem 1 (2 pts)

### Part (a) (1 pt)

Here is the expression pseudo random number generator that was used on a particular version of the VAX VMS operating system. It produces numbers between 0 and $2^{31} - 1$

$$r(s) = \left( s \times (2^{16} + 3)\right) \mod 2^{31} $$

This particular random number generator was notorious for producing poor quality of random numbers.  

Implement a version of this random number generator that returns a single value and then use that to return `n` random integers

```{r}
bad_rand_int <- function(seed) {
  # Note: mod is "%%" in R
  return((seed*(2^16+3)) %% 2^31)
}
bad_rand_ints <- function(seed, n) {
  rngs <- numeric(length = n)
  for (i in 1:n){
    seed <- rngs[i] <- bad_rand_int(seed)
  }
 return(rngs)
}
```

Using your function `bad_rand_ints`, starting from a seed of 406 generate 10 random values. Do you notice anything that would suggest these numbers are not uniformly random over the set 0 to $2^{31} - 1$? (Hint: think of classes of numbers you would expect to see in known proportions -- do you see these classes appearing correctly?)
```{r echo=FALSE}
(randoms <- bad_rand_ints(406,10))
```
**Answer**: According to Benford's law, the leading digit should appear in a way that is decreasing in proportion from 1 to 9. However, there are two numbers with leading digit 7, which seems suspicious.  

Now write a function that uses `bad_rand_ints` to produce a list containing `n` psuedorandom $U(0,1)$ values.

```{r}
bad_rand_u01 <- function(seed, n) {

  bad_ints <- bad_rand_ints(seed, n)
  mim <- min(bad_ints)
  mxm <- max(bad_ints)
  uniforms <- map_dbl(bad_ints, ~(.x/(2^31-1)))
  return(uniforms)
}
```

Draw 10 random numbers using `bad_rand_u01`. Did the pattern you saw persist?
```{r}
bad_rand_u01(406,10)
```
### Part (b) (1 pt)
Draw 10,000 random numbers and make a Q-Q plot compared to the uniform distribution. Does it seem to produce $U(0,1)$ random numbers?

```{r}
## This next block of code will only need to run once to install an additional package if you don't have it.
if (!require("lattice")) { 
  install.packages("lattice")  
}
```
```{r}
random_10000 <- bad_rand_u01(406,10000)
ggplot(data.frame(sample = random_10000),aes(sample = sample))+geom_qq(distribution = qunif)+geom_qq_line(distribution = qunif)
```   

**Answer**: It seems that the points are almost uniformly distributed between (0,1) in the graph ,so it seem to produce U(0,1) random generated numbers.  

While a sequence of PRNGs might look OK when viewed *marginally*, viewing the sequences as points in a space can be useful to detect non-random patterns.

When you are ready, the following code will produce a plot when called on your collection of 10,000 $U(0,1)$ random numbers from the previous part. What pattern do you see?

```{r}
plot_bad_rands <- function(prngs) {
  if (length(prngs) %% 10 != 0) {
    stop("We must have random numbers in a multiple of 10")
  }
  # group the random numbers into groups of 10, but only use the first 3 in each group
  m <- matrix(prngs, ncol = 10, byrow = TRUE)[, 1:3]
  
  # label the columns 
  colnames(m) <- c("X", "Y", "Z")
  
  cloud(Z ~ X + Y, as.data.frame(round(m, 3)), pch = 20, cex = 0.1)
}

 plot_bad_rands(random_10000) # uncomment and add your data here
```  
  
**Answer**: It seems that on the left side of the cube, some data are in some linear patterns, forming several distinguishable lines, while data on the right side of the cube seems more random.  

## Problem 2 (1 pt)

In class we proved that the inversion method works in the continuous case. Prove that it works in the discrete case as well. Two useful facts:

- For any discrete on any domain, there is is a one-to-one mapping from that domain to the integers. So without loss of generality, we can assume all discrete RVs hav the integers as their support.
- Let the discrete random variable $X$ be defined on the set $\mathcal{X}$. If $P(X = x) = P(Y = x)$ for all $x \in \mathcal{X}$, then $X$ and $Y$ have the same distribution. 

**Answer**: In class, we already shown $Pr(Q(U)<=X)=F(X)=Pr(X<=x)$. In the case of discrete random variable, we have $F(x)=\sum_{i=1}^{n}x_{i}P(X=x_{i})$. Therefore,   
\[
\begin{align}
Pr(Q(U)=x)&=Pr((x:F(x-1)<U<=F(x))=x)\\
&=Pr(\{x:\sum_{i=1}^{x-1}P(X_{i})<U<=\sum_{i=1}^{x}P(X_{i})\}=x) \\
&=\sum_{i=1}^{x}P(X=x_{i}) - \sum_{i=1}^{x-1}P(X=x_{i}) \\
&=Pr(X=x) \\

\end{align}
\]  
For all $x \in \mathcal{X}$, which indicate $Q_{x}(U)$ has the same distribution as P(X=x)  

## Problem 3 (3 pts)

Recall the exponential distribution with mean $\theta^{-1}$ has density:
$$f(x) = \theta e^{- \theta x}$$

### Part (a) (1 pt)

Find the quantile function of an exponential with rate parameter $\theta$.  

$F(x) = \int\theta e^{- \theta x} dx = 1 - e^{- \theta x}$  
\[
\begin{align}
F(x) &= 1 - e^{- \theta x} \\
x &= 1 - e^{- \theta F^{-1}(x)} \\
e^{- \theta F^{-1}(x)} &= 1 - x \\
- \theta F^{-1}(x) &= \log(1 - x) \\
F^{-1}(x) &= \frac{\log(1 - x)}{-\theta}
\end{align}
\]  

### Part (b) (1 pt)

You may recall that the mean and standard deviation of $\text{Exp}(\theta)$ is $\mu = \sigma = 1/\theta$. But what is the skew?
$$\gamma = \text{E}\left[\left(\frac{X - \mu}{\sigma}\right)^3\right]$$
Use the quantile function from (a) to sample 10,000 exponential random variables with rate parameter 2. Estimate $\gamma$ and provide a 99.9% confidence interval.
```{r}
quantile <- function(x,theta){
  return(log(1-x)/-(theta))
}
expect_gamma <- function(x){
  return (((x-(1/2))/(1/2))^3)
}
theta <- 2

sample = runif(10000)
sampled_exp <- map_dbl(sample,~quantile(.x,theta))
gammas <- map_dbl(sampled_exp,~expect_gamma(.x))
(gamma <- mean(gammas))

(interval <- t.test(gammas, conf.level = 0.999)$conf.int)
```
### Part (c) (1 pt)

Using your results from (a) to prove that if $U \sim U(0, 1)$ then,
$$- \frac{1}{\theta} \log(U) \sim \text{Exp}(\theta), \theta > 0$$
(Where $\log$ is the natural logarithm as always in this class.)

**Answer**: At the beginning we have $U \sim U(0, 1)$, then
\[
\begin{align}
U &\sim U(0, 1) \\
log(U) &\sim log(U(0, 1)) \\
-\frac{1}{\theta}log(U) &\sim -\frac{1}{\theta}log(U(0, 1)) \\
\end{align}
\]  
Since we know U(0,1) is uniform, so the distribution between $-\frac{1}{\theta}log(U)$ and $-\frac{1}{\theta}log(1-U)$ is essentially the same distribution. From part a, we already show that $-\frac{1}{\theta}log(1-U)$ is the inversion method to generate exponential data. Thus, we can conclude $-\frac{1}{\theta}log(U) \sim \text{Exp}(\theta), \theta > 0$  

## Problem 4 (4 pts)

The standard Normal distribution:
$$f(x) = \frac{1}{\sqrt{2\pi}} \exp\{ -x^2/2 \}$$
does not have a closed form quantile function, so it would be difficult to apply the inversion method. Instead, we can use a transformation method that still only uses $U(0,1)$ random variables.

### Part (a) (1 pt)

Consider two **independent** standard Normal variables $X$ and $Y$. We can think of these as points on a Cartesian plane:

```{r}
xy <- ggplot(data.frame(x = rnorm(50), y = rnorm(50)), aes(x = x, y = y)) + geom_point()
print(xy)
```


We could also think about these points using **polar coordinates** based on a radius (distance from the origin) $R = \sqrt{X^2 + Y^2}$ and angle (from 0 to $2\pi$) such that $\cos(A) = X / R$ and $\sin(A) = Y / R$:
```{r}
xy + geom_segment(aes(xend = 0, yend = 0))
```

What is $R^2$? [Use this list of common relationships](https://en.wikipedia.org/wiki/Relationships_among_probability_distributions) to express $R^2$ as an **exponential random variable** (since exponentials can be parameterized using **rate** or **mean**, use the rate parameterization $W \sim \text{Exp}(\theta)$, $E(X) = 1/\theta$).  

**Answer**: $R^2 = X^2 + Y^2$. $R^2$ is a Chi-squared. $R^2 \sim Exp(\frac{1}{2})$ with $\theta = \frac{1}{2}$ and $E(X)=\frac{1}{\theta}=\frac{1}{\frac{1}{2}}=2$  

### Part (b) (1 pt)

Show that the joint distribution for two independent standard Normal random variables is proportional to the joint distribution for a $A \sim U(0, 2\pi)$ and the $R^2$ you found in (a), where $A$ and $R^2$ are independent. 

**Answer**: Let $W_{1},W_{2}$ be two independent standard normal random variables. Then their joint distribution is $f(w_{1},w_{2})=(W_{1})^2=(W_{2})^2$ since both have the same distribution. Then their joint distribution is a chi-squared distribution by definition. From previous part we know $R^2 = X^2 + Y^2$ is a chi-squared distribution with degree freedom of 2. The joint distribution of $A$ and $R^2$ is $f(u,x,y)=U(X^2+Y^2),U\in(0,2\pi)$. Therefore, $f(w_{1},w_{2})$ and $f(u,x,y)$ have the same distribution (both chi-squared) with difference in constant.  

### Part (c) (1 pt)

Use the result from 3(c) that $-(1/\theta) \log(U) \sim \text{Exp}(\theta)$ along with the identity $X = R \cos(A)$ to show how to generate one standard Normal random variable from two independent $U(0,1)$ random variables. (Interesting note, you can also use $Y = R \sin(A)$ to get a second standard Normal, which is also independent, but this is not necessary to show.)

**Answer**: Suppose we have two independent uniform random variable $U_{1},U_{2}\sim U(0,1)$. Using result from 3c, we can generate an exponential random variable $\text{Exp}(\theta)$ by doing transformation $-(1/\theta) \log(U_{1})$ for a given $\theta$. Also since $R^2 \sim Exp(\frac{1}{2})$, then we can find a way to express $R$ from our exponential random variable. To express $\cos(A)$, notice that $A \sim U(0, 2\pi)$. Then we can increment our $U_{2}$ to a factor of $2\pi$, then $U_{2}\sim U(0,2\pi)$. Then applying the $cos$ function on it in order to get $\cos(A)$. Therefore, with two expression combined, we can generate one standard Normal random variable $X$, where $X = R \cos(A)$.  

### Part (d) (1 pt)

Implement your part (c) in R. Demonstrate your results using a quantile-quantile plot (replacing `rnorm` with your solution.)  
**The geom_qq_line function doesn't work well on my laptop. When I run chunk code, it shows up. But when I knit, it does not shows up. It appears to be a rare bug.**

```{r}
exp_func <- function(u,theta){
  return (sqrt((log(u)/-theta)))
}

exp_10000 <- map_dbl(runif(10000), ~exp_func(.x,0.5))
cos_10000 <- cos(runif(10000,min = 0,max = 2*pi))
norm_10000 <- exp_10000*cos_10000

ggplot(data.frame(x = norm_10000), aes(sample = x)) + geom_qq(distribution = qnorm) + geom_qq_line(distribution = qnorm)
```


