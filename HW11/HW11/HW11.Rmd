---
title: "Homework 11"
author: "Jiawei Hao, hjiawei"
date: "Due Dec. 10 at 11:59pm"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(20324929)
```

## Question 1 (6 points)

### Part (a) (1 point)

Read sections 1 to 3.1, and 4 to 4.1 of "Environmental Kuznets Curve Hypothesis: A Survey" (of course, you may read more, but these sections contain the most releveant information for this week's homework). Briefly summarize the idea of the **Environmental Kuznets Curve** (EKC). Specifically, what kind of relationship does it posit between economic development and environmental impact, for example CO$_2$ emission?

**Answer**: EKC points to the relationship between economic growth (GDP per capita) and environment pollution. The paper emphasizes that there exists an inverted-U shape relationship between the two: as developing country start to grow rapidly, people care less about environment and care more about the money. When the economy growth pass a certain threshold, more people are willing to have a cleaner environment. As a result, environment deteriorate at first and become more sustainable later in the rapid economic growth. In terms of CO$_2$, it usually means developing countries will emit significant amount of CO$_2$ when they start to develop, as their economies become better, there will be willingness and policies to reduce CO$_2$ emission.  

### Part (b) (1 points)

```{r}
load("world_bank.rda") # loads world_bank table
```

These data contain three different variables measured on countries and other entities in 2014. Describe how you can use these data to investigate the EKC. (Note: countries have vastly different populations. How will you account for that?)

**Answer**: To standarized by population, I can use GDP, CO$_2$ population to find GDP per capita and CO$_2$ per capita. Then we may use ordinary least square model to investigate the relationship between these two variables. Additionally, we can plot a graph between GDP per capita (represent economic growth) and CO$_2$ emission (represent environment condition). Then we observe if the plot follows some forms of inverted-U shape.  

### Part (c) (2 points)

Section 4.1 of the EKC paper includes a specification for a model that relates country income to CO2 per capita. For our purposes, we will fix a single observation per country ($t = 1$ for all observations) and not include any other predictors (labeled $z$ in the model).

Fit an OLS model using the specification given in equation (1) on page 440. Interpret your results using the listing of possible outcomes given on pages 440 and 441 by applying hypothesis tests that the parameters equal zero (you may use `summary(model)` to get the p-values of the tests). Do you think there is evidence to support the EKC theory? Can we rule out the EKC theory?

```{r}
world_bank_clean <- na.omit(world_bank)
GDPs <- world_bank_clean$GDP
population <- world_bank_clean$Population
co2 <- world_bank_clean$CO2

gdp_per_cap <- GDPs/population
co2_per_cap <- co2/population

design_matrix <- as.matrix(cbind(gdp_per_cap,(gdp_per_cap)^2,(gdp_per_cap)^3))

(beta1 <- lm(co2_per_cap ~ gdp_per_cap + design_matrix[,2] + design_matrix[,3]))
summary(beta1)
```
**Answer**: Using p-value, we can see strong evidence suggest that $\beta_1$ and $\beta_2$ are not 0 and not strong enough evidence to reject $\beta_3 = 0$ since $p=0.08370$ is not very significant. Thus, the betas we find which does not satisfy EKC conditions described in relationship (iv). So the evidence does not support EKC theory, we can rule out EKC theory.  

### Part (d) (2 points)

As an alternative specification, we could use kernel smoothing to estimate the conditional mean of CO2 emissions given GDP. Using leave one out cross validation, select a bandwidth to use with a Gaussian/Normal kernel. Plot your result and interpret the curve with respect to evidence for or against the EKC.

```{r}
bp_smooth <- function(index, width){
  Yi <- ksmooth(bandwidth = width, x = gdp_per_cap[-index], y = co2_per_cap[-index], kernel = "normal", x.points = gdp_per_cap[-index])
  loss <- Yi$y - co2_per_cap[index]
  return(loss)
}
loss <- function(width){
  mean(sapply(1:nrow(world_bank_clean), bp_smooth, width = width))
}

widths <- seq(1, 150000, length.out = 300)
width_lose1 <- sapply(widths, loss)
(turning_point <- widths[which.min(width_lose1)])

smoothed_line <- ksmooth(x = gdp_per_cap, y = co2_per_cap, bandwidth = turning_point, n.points = 1000)
plot(x = gdp_per_cap, y=co2_per_cap)
lines(x = smoothed_line$x, y = smoothed_line$y)
```  

**Answer**: From our smoothed curve, we can see a trend of going up first, and then flatten out to a gradually down trend. It's not a good inverted-u shape but we do can see a pattern for it. So this should be an evidence that supports EKC.  


## Question 2 (4 points)

While we can motivate ordinary least squares (OLS) as finding the maximum likelihood estimates for $Y \sim N(\beta'x, \sigma^2)$, OLS has wider applicability and still nice properties.

When we model $E(Y \mid x) = \beta'x$ and assume a constant variance, ordinary least squares OLS is the best linear unbiased estimator of the $\beta$ parameters (i.e., among all methods that are linear functions of $Y$ and would have unbiased estimates of the parameters, OLS has the smallest variance.)

### Part (a) (1 pt)

Suppose we have the simple model

$$E(Y \mid x) = \beta x$$
(i.e., there is no intercept term.)

Prove that the OLS estimate of $\beta$ is given by:

$$\hat \beta = \sum_{i = 1}^n w_i Y_i, \quad w_i = \frac{x_i}{\sum_{i=1}^n x_i^2}$$

and show that this is **unbiased for $\beta$**.

**Answer**: Under OLS, we can build a system of equations using $X^TX\beta=X^TY$. For data sets $(x_1...x_n)$ and corresponding $(Y_1...Y_n)$, we can then have 

\begin{align}
(x_1...x_n)^T(x_1...x_n)\beta &= (x_1...x_n)^T(Y_1...Y_n) \\
\sum_{i=1}^n x_i^2\beta &= \sum_{i=1}^n x_iY_i \\
\hat\beta &= \sum_{i=1}^{n}\frac{x_i}{\sum_{j=1}^n x_j^2}Y_i

\end{align}

  
To find its bias:  
  

\begin{align}
E[\hat\beta] &= \sum_{i = 1}^n w_i Y_i, \quad w_i \\
&= E[w_1Y_1+...+w_nY_n] \\
&= w_1E[Y_1]+....+w_nE[Y_n] \\
&= w_1E[\beta x_1]+...+w_1E[\beta x_n] \\
&= E[\beta](w_1x_1+...+w_nx_n) \\
&= E[\beta](\frac{x_1^2}{x_1^2+...+x_n^2} +...+ \frac{x_n^2}{x_1^2+...+x_n^2}) \\
&= E[\beta] \\
&= \beta


\end{align}
 
Then $Bias = E[\hat \beta] - \beta = 0$. $\hat \beta = \sum_{i = 1}^n w_i Y_i$ is an unbiased estimate of $\beta$.  


Under the same model, show that estimator 

$$\tilde \beta = \sum_{i=1}^n w_i Y_i, \quad w_i = \frac{1}{n x_i}$$
is unbiased for $\beta$.

**Answer**: 

\begin{align}
E[\tilde\beta] &= \sum_{i = 1}^n w_i Y_i, \quad w_i \\
&= E[w_1Y_1+...+w_nY_n] \\
&= w_1E[Y_1]+....+w_nE[Y_n] \\
&= w_1E[\beta x_1]+...+w_1E[\beta x_n] \\
&= E[\beta](w_1x_1+...+w_nx_n) \\
&= E[\beta](\frac{1}{nx_1}x_1 +...+ \frac{1}{nx_n}x_n) \\
&= E[\beta] \\
&= \beta

\end{align}

Then $Bias = E[\tilde \beta] - \beta = 0$. $\tilde \beta = \sum_{i = 1}^n w_i Y_i$ is an unbiased estimate of $\beta$. 

### Part (b) (1 pt)

We can also consider **biased estimators** that have lower variance. One such estimator is called **ridge regression** and the estimator is given by

$$\bar \beta = (X'X + \lambda I)^{-1} X'Y$$
where $\lambda$ is some positive value and $I$ is the identity matrix.

Express the ridge regression estimator as $\sum_{i=1}^n w_i Y_i$ for some set of weights $w_i$ for the model of (a) and show that for any $\lambda > 0$, $\bar \beta$ and $\hat \beta$ will have **same sign** and $|\bar \beta| < |\hat \beta|$.

(Note: ridge regression is known as a **shrinkage estimator** in that in **shrinks the estimate of $\beta$ towards zero**. This can be useful if one is considering a model with many $\beta$ parameters for variables on roughly the same scale, but you only think a few of the variables influence the outcome. Ridge regression, and other shrinkage estimators, will force small estimates even smaller, letting the important predictors shine through.)

**Answer**:

\begin{align}
\bar \beta &= (X'X + \lambda I)^{-1} X'Y \\
&= (\sum_{i=1}^n X_i \cdot X_i + \lambda)^{-1} \sum_{i=1}^{n} X_i \cdot Y_i \\
&= \frac{1}{\sum_{i=1}^n X_i^2 + \lambda}\sum_{i=1}^{n} X_i Y_i \\
&= \sum_{i=1}^{n} \frac{X_i}{\sum_{i=1}^n X_i^2 + \lambda} Y_i \\
&= \sum_{i=1}^n w_i Y_i
\end{align}  

Where $w_i = \frac{X_i}{\sum_{i=1}^n X_i^2 + \lambda}$.  
To compare sign of $\bar \beta$ and $\hat \beta$, we can compare $\frac{X_i}{\sum_{i=1}^n X_i^2 + \lambda} Y_i$ to $\frac{x_i}{\sum_{i=1}^n x_i^2}Y_i$. Notice that the numerator are the same. Denominator are $\sum_{i=1}^n X_i^2$ and $\sum_{i=1}^n X_i^2 + \lambda$, they will both be greater than 0 since $\sum X^2$ and $\lambda$ are always greater than 0. So no matter what sign numerator has, they will always have the same sign.  
Additionally, $|\bar \beta| < |\hat \beta|$ because in the summation for $\bar \beta$, there is an extra term in its denominator, which is $\lambda$, that is added to the summation of $x^2$. Since the denominator for both are always positive values adding each other and the denominator of $\bar \beta$ is larger since it added $\lambda$, so $|\bar \beta| < |\hat \beta|$.  


### Part (c) (2 pt)

Estimate the bias and variance of the sampling distributions for $\hat \beta$,  $\tilde \beta$, and $\bar \beta$ when $\lambda = 1$ and $\lambda = 1000$ when

- $\beta = 2$
- $n = 20$
- $x_i \sim 1 + \text{Exp}(1/10)$
- $Y_i \mid x_i \sim U(\beta x_i - 50, \beta x_i + 50)$

Use 10,000 Monte Carlo samples.

```{r}
beta <- 2
n <- 20
lambda_1 <- 1
lambda_1000 <- 1000

#beta_hat
beta_hats <- replicate(10000, {
  Xs <- 1 + rexp(n = 20, rate = 1/10)
  sum_square_x <- sum(Xs^2)
  Ws <- Xs/sum_square_x
  Ys <- runif(n = 20, beta*Xs - 50, beta*Xs + 50)
  beta_hat <- sum(Ws * Ys)
})

E_beta_hat <- mean(beta_hats)
(bias_beta_hat <- E_beta_hat - beta)
(var_beta_hat <- var(beta_hats))

#beta_tilde
beta_tildes <- replicate(10000,{
  Xs <- 1 + rexp(n = 20, rate = 1/10)
  Ys <- runif(n = 20, beta*Xs - 50, beta*Xs + 50)
  Ws <- 1 / (n*Xs)
  beta_tilde <- sum(Ws * Ys)
})

E_beta_tilde <- mean(beta_tildes)
(bias_beta_tilde <- E_beta_tilde - beta)
(var_beta_tilde <- var(beta_tildes))

#beta_bar, lambda = 1
beta_bars <- replicate(10000, {
  Xs <- 1 + rexp(n = 20, rate = 1/10)
  Ys <- runif(n = 20, beta*Xs - 50, beta*Xs + 50)
  Ws <- Xs / (sum(Xs^2 + lambda_1)) 
  beta_bar <- sum(Ws * Ys)
})

E_beta_bar <- mean(beta_bars)
(bias_beta_bar <- E_beta_bar - beta)
(var_beta_bar <- var(beta_bars))

#beta_bar, lambda = 1000
beta_bars_1000 <- replicate(10000, {
  Xs <- 1 + rexp(n = 20, rate = 1/10)
  Ys <- runif(n = 20, beta*Xs - 50, beta*Xs + 50)
  Ws <- Xs / (sum(Xs^2 + lambda_1000)) 
  beta_bar <- sum(Ws * Ys)
})

E_beta_bar_1000 <- mean(beta_bars_1000)
(bias_beta_bar_1000 <- E_beta_bar_1000 - beta)
(var_beta_bar_1000 <- var(beta_bars_1000))
```
